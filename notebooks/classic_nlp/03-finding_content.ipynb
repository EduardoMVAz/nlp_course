{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content-based search\n",
    "\n",
    "The Internet has brought forward a marvelous source of information. But - simply knowing that we *have* information is just not enough to *use* this information. For example, we *know* that, somewhere on the Internet, there is a book on Natural Language Processing. But, how can we find this book?\n",
    "\n",
    "In this notebook, we are going to work with the following use case (which was also approached in [Amami et al., \"An LDA-Based Approach to Scientific Paper Recommendation\",Natural Language Processing and Information Systems, 2016 ](http://link.springer.com/10.1007/978-3-319-41754-7_17), based on ideas by [Griffiths and Steyvers, \"Finding Scientific Topics\", Proc. Natl. Acad. Sci. U.S.A., 2004](https://doi.org/10.1073/pnas.0307752101).\n",
    "\n",
    "Suppose a scientist is writing an article. Articles usually start with a session called \"abstract\", which summarizes the contents of the whole paper. We want our system to get the abstract we are working with, and then find possible articles we could work with.\n",
    "\n",
    "We will start by simulating our data with a subset of an ArXiv dataset available at Kaggle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import os\n",
    "import kagglehub\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "    \n",
    "path = kagglehub.dataset_download(\"tiagoft/arvix-data-filtered-for-cs-only-data\")\n",
    "path = Path(path)\n",
    "df = pd.read_csv(path / 'arxiv-metadata-oai-snaptshot-cs-only.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_title = \"Enhancing Autonomous Agents with Multimodal Generative AI for Improved Human-AI Collaboration\"\n",
    "sample_abstract = \"\"\"The integration of multimodal generative AI into autonomous agents presents a significant advancement in human-AI collaboration. \n",
    "This study explores the development of autonomous agents capable of processing and generating various data types,\n",
    "including text-to-image and image-to-audio conversions. By leveraging multimodal generative AI, these agents can interpret and generate \n",
    "content across different modalities, enhancing their ability to interact with humans in more natural and intuitive ways.\n",
    "We propose a novel framework that combines generative AI with transfer learning techniques to enable autonomous agents to adapt \n",
    "knowledge acquired from one context to another with minimal additional data. Our experiments demonstrate that this approach significantly\n",
    "improves the agents' performance in tasks requiring human-AI collaboration, such as virtual reality environments and smart city applications.\n",
    "The results highlight the potential of multimodal generative AI to revolutionize human-AI interaction, paving the way for more immersive \n",
    "and adaptive collaborative experiences.\n",
    "\"\"\"\n",
    "sample_keywords = [\"autonomous agents\", \"multimodal generative AI\", \"human-AI collaboration\", \"transfer learning\", \"virtual reality\", \"smart city applications\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: search by keyword\n",
    "\n",
    "Searching by keywords is somewhat simple because we can simply use an inverted index. In fact, online search engines usually implement inverted index.\n",
    "\n",
    "Use your inverted index to try to find other, relevant articles within our dataset using the keywords provided by the abstract's author."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\emend\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(set, {'nlp': {0}, 'is': {0}, 'so': {0}, 'cool': {0}})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.stem import  PorterStemmer\n",
    "from collections import defaultdict\n",
    "\n",
    "nltk.download('wordnet')\n",
    "iidx = defaultdict(set)\n",
    "\n",
    "def add_to_index(iidx, new_text, idx, lemmatize=False):\n",
    "    all_words = new_text.lower()\n",
    "    all_words = re.findall(r'\\b\\w\\w+\\b', all_words)\n",
    "\n",
    "    if lemmatize:\n",
    "        stemmed = PorterStemmer()\n",
    "        all_words = [stemmed.stem(word) for word in all_words]\n",
    "\n",
    "    for word in all_words:\n",
    "        iidx[word].add(idx)\n",
    "    return iidx\n",
    "\n",
    "def find_in_index(iidx, words, lemmatize=False):\n",
    "    if lemmatize:\n",
    "        stemmed = PorterStemmer()\n",
    "        words = [stemmed.stem(word) for word in words]\n",
    "    \n",
    "    docs = [iidx[word] for word in words]\n",
    "    d = docs[0]\n",
    "    for i in range(len(docs)):\n",
    "        d = d.intersection(docs[i])\n",
    "    return d\n",
    "\n",
    "iidx = add_to_index(iidx, \"NLP is so cool!\", 0, lemmatize=True)\n",
    "iidx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'abstract'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\emend\\OneDrive\\Área de Trabalho\\NLP\\env\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'abstract'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m iidx \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28mset\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, content \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mabstract\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m):\n\u001b[0;32m      4\u001b[0m     iidx \u001b[38;5;241m=\u001b[39m add_to_index(iidx, content, idx)\n",
      "File \u001b[1;32mc:\\Users\\emend\\OneDrive\\Área de Trabalho\\NLP\\env\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\emend\\OneDrive\\Área de Trabalho\\NLP\\env\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'abstract'"
     ]
    }
   ],
   "source": [
    "iidx = defaultdict(set)\n",
    "\n",
    "for idx, content in enumerate(df[\"abstract\"]):\n",
    "    iidx = add_to_index(iidx, content, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{40962,\n",
       " 49158,\n",
       " 49160,\n",
       " 57360,\n",
       " 49175,\n",
       " 49184,\n",
       " 57400,\n",
       " 57402,\n",
       " 41022,\n",
       " 57409,\n",
       " 24644,\n",
       " 49226,\n",
       " 57426,\n",
       " 57431,\n",
       " 32859,\n",
       " 57439,\n",
       " 57442,\n",
       " 41067,\n",
       " 32882,\n",
       " 41079,\n",
       " 16518,\n",
       " 49289,\n",
       " 41135,\n",
       " 41143,\n",
       " 49337,\n",
       " 49340,\n",
       " 57549,\n",
       " 49358,\n",
       " 32980,\n",
       " 49365,\n",
       " 57556,\n",
       " 57560,\n",
       " 33000,\n",
       " 24816,\n",
       " 41200,\n",
       " 57611,\n",
       " 57613,\n",
       " 57623,\n",
       " 49433,\n",
       " 57632,\n",
       " 49447,\n",
       " 57642,\n",
       " 33084,\n",
       " 33096,\n",
       " 33097,\n",
       " 33105,\n",
       " 33106,\n",
       " 41299,\n",
       " 33116,\n",
       " 57709,\n",
       " 57713,\n",
       " 16762,\n",
       " 49540,\n",
       " 57739,\n",
       " 33179,\n",
       " 41378,\n",
       " 57764,\n",
       " 49580,\n",
       " 57777,\n",
       " 25012,\n",
       " 49588,\n",
       " 49592,\n",
       " 41401,\n",
       " 49598,\n",
       " 57790,\n",
       " 57801,\n",
       " 41429,\n",
       " 57824,\n",
       " 49637,\n",
       " 16883,\n",
       " 41465,\n",
       " 25086,\n",
       " 41474,\n",
       " 49667,\n",
       " 49674,\n",
       " 49677,\n",
       " 57878,\n",
       " 25122,\n",
       " 41509,\n",
       " 57894,\n",
       " 8743,\n",
       " 49705,\n",
       " 8760,\n",
       " 41528,\n",
       " 41543,\n",
       " 49741,\n",
       " 57935,\n",
       " 16976,\n",
       " 49751,\n",
       " 41572,\n",
       " 49765,\n",
       " 41580,\n",
       " 41587,\n",
       " 49790,\n",
       " 49791,\n",
       " 49794,\n",
       " 647,\n",
       " 49805,\n",
       " 58002,\n",
       " 25238,\n",
       " 41623,\n",
       " 666,\n",
       " 49818,\n",
       " 58014,\n",
       " 58015,\n",
       " 49826,\n",
       " 58019,\n",
       " 33444,\n",
       " 41637,\n",
       " 25260,\n",
       " 8888,\n",
       " 33464,\n",
       " 49850,\n",
       " 33475,\n",
       " 33477,\n",
       " 49887,\n",
       " 33506,\n",
       " 58088,\n",
       " 8948,\n",
       " 33525,\n",
       " 41718,\n",
       " 41720,\n",
       " 58114,\n",
       " 8965,\n",
       " 58122,\n",
       " 8977,\n",
       " 33554,\n",
       " 49937,\n",
       " 49940,\n",
       " 58130,\n",
       " 49946,\n",
       " 25383,\n",
       " 49961,\n",
       " 9015,\n",
       " 33600,\n",
       " 58177,\n",
       " 49999,\n",
       " 25429,\n",
       " 41814,\n",
       " 58198,\n",
       " 50011,\n",
       " 50012,\n",
       " 41840,\n",
       " 33652,\n",
       " 41849,\n",
       " 41852,\n",
       " 9088,\n",
       " 50057,\n",
       " 9098,\n",
       " 58250,\n",
       " 41880,\n",
       " 25505,\n",
       " 9124,\n",
       " 50094,\n",
       " 41912,\n",
       " 50112,\n",
       " 41921,\n",
       " 33730,\n",
       " 58320,\n",
       " 58322,\n",
       " 58328,\n",
       " 58330,\n",
       " 33760,\n",
       " 50144,\n",
       " 58341,\n",
       " 58343,\n",
       " 33771,\n",
       " 33779,\n",
       " 50164,\n",
       " 58355,\n",
       " 17398,\n",
       " 50180,\n",
       " 9221,\n",
       " 33798,\n",
       " 41996,\n",
       " 50188,\n",
       " 17427,\n",
       " 58388,\n",
       " 33828,\n",
       " 42026,\n",
       " 50222,\n",
       " 58422,\n",
       " 58432,\n",
       " 25667,\n",
       " 33868,\n",
       " 33877,\n",
       " 58463,\n",
       " 33897,\n",
       " 25708,\n",
       " 58476,\n",
       " 50287,\n",
       " 33904,\n",
       " 42097,\n",
       " 17522,\n",
       " 9339,\n",
       " 58492,\n",
       " 58496,\n",
       " 50316,\n",
       " 42132,\n",
       " 9372,\n",
       " 1183,\n",
       " 33955,\n",
       " 25764,\n",
       " 50344,\n",
       " 50345,\n",
       " 1197,\n",
       " 33969,\n",
       " 58558,\n",
       " 50371,\n",
       " 33992,\n",
       " 42193,\n",
       " 50403,\n",
       " 17636,\n",
       " 9447,\n",
       " 34024,\n",
       " 58601,\n",
       " 58603,\n",
       " 58604,\n",
       " 42224,\n",
       " 50425,\n",
       " 25850,\n",
       " 9470,\n",
       " 58625,\n",
       " 58638,\n",
       " 42262,\n",
       " 42263,\n",
       " 58668,\n",
       " 25907,\n",
       " 50490,\n",
       " 58703,\n",
       " 34142,\n",
       " 58735,\n",
       " 42356,\n",
       " 58783,\n",
       " 34213,\n",
       " 50599,\n",
       " 42428,\n",
       " 50628,\n",
       " 17863,\n",
       " 58826,\n",
       " 58830,\n",
       " 50643,\n",
       " 42469,\n",
       " 42472,\n",
       " 1516,\n",
       " 17922,\n",
       " 58888,\n",
       " 42506,\n",
       " 58904,\n",
       " 17945,\n",
       " 26153,\n",
       " 42541,\n",
       " 42549,\n",
       " 50744,\n",
       " 50747,\n",
       " 50756,\n",
       " 34373,\n",
       " 42573,\n",
       " 1625,\n",
       " 26206,\n",
       " 50786,\n",
       " 58983,\n",
       " 18058,\n",
       " 26250,\n",
       " 50826,\n",
       " 34447,\n",
       " 42640,\n",
       " 59025,\n",
       " 59048,\n",
       " 50859,\n",
       " 50861,\n",
       " 50863,\n",
       " 50864,\n",
       " 59058,\n",
       " 1722,\n",
       " 34491,\n",
       " 42695,\n",
       " 34508,\n",
       " 50897,\n",
       " 59109,\n",
       " 50922,\n",
       " 50923,\n",
       " 42740,\n",
       " 50935,\n",
       " 59137,\n",
       " 50946,\n",
       " 59139,\n",
       " 34564,\n",
       " 34567,\n",
       " 59191,\n",
       " 34623,\n",
       " 34626,\n",
       " 51012,\n",
       " 42821,\n",
       " 42831,\n",
       " 42839,\n",
       " 34651,\n",
       " 42846,\n",
       " 59232,\n",
       " 42851,\n",
       " 51055,\n",
       " 59252,\n",
       " 51062,\n",
       " 42873,\n",
       " 34685,\n",
       " 34688,\n",
       " 42882,\n",
       " 59280,\n",
       " 59284,\n",
       " 59295,\n",
       " 34725,\n",
       " 59301,\n",
       " 51112,\n",
       " 18352,\n",
       " 42931,\n",
       " 18356,\n",
       " 42934,\n",
       " 42970,\n",
       " 34780,\n",
       " 2026,\n",
       " 51188,\n",
       " 43001,\n",
       " 43009,\n",
       " 51206,\n",
       " 59398,\n",
       " 59411,\n",
       " 51224,\n",
       " 51231,\n",
       " 51243,\n",
       " 10285,\n",
       " 43053,\n",
       " 43055,\n",
       " 43058,\n",
       " 43063,\n",
       " 51263,\n",
       " 59459,\n",
       " 59462,\n",
       " 59466,\n",
       " 34904,\n",
       " 59481,\n",
       " 10331,\n",
       " 59484,\n",
       " 43105,\n",
       " 43108,\n",
       " 51300,\n",
       " 26731,\n",
       " 34923,\n",
       " 59502,\n",
       " 18564,\n",
       " 34950,\n",
       " 26768,\n",
       " 59544,\n",
       " 59545,\n",
       " 34989,\n",
       " 43188,\n",
       " 51381,\n",
       " 59578,\n",
       " 59581,\n",
       " 51392,\n",
       " 18627,\n",
       " 35011,\n",
       " 59592,\n",
       " 35020,\n",
       " 59597,\n",
       " 35032,\n",
       " 59609,\n",
       " 35057,\n",
       " 26880,\n",
       " 35076,\n",
       " 35078,\n",
       " 51466,\n",
       " 51480,\n",
       " 35097,\n",
       " 2331,\n",
       " 59675,\n",
       " 43295,\n",
       " 26913,\n",
       " 59682,\n",
       " 26915,\n",
       " 43299,\n",
       " 59687,\n",
       " 59706,\n",
       " 26942,\n",
       " 43329,\n",
       " 35144,\n",
       " 26963,\n",
       " 51539,\n",
       " 26966,\n",
       " 26980,\n",
       " 43376,\n",
       " 43383,\n",
       " 43387,\n",
       " 18814,\n",
       " 59775,\n",
       " 2432,\n",
       " 51584,\n",
       " 51591,\n",
       " 51599,\n",
       " 18832,\n",
       " 51600,\n",
       " 59804,\n",
       " 59817,\n",
       " 2475,\n",
       " 35245,\n",
       " 43439,\n",
       " 59824,\n",
       " 59825,\n",
       " 18873,\n",
       " 59843,\n",
       " 18885,\n",
       " 59850,\n",
       " 51659,\n",
       " 35276,\n",
       " 51661,\n",
       " 59859,\n",
       " 51674,\n",
       " 43493,\n",
       " 43494,\n",
       " 59894,\n",
       " 35324,\n",
       " 43549,\n",
       " 51746,\n",
       " 35366,\n",
       " 43559,\n",
       " 43562,\n",
       " 35374,\n",
       " 43584,\n",
       " 27206,\n",
       " 35405,\n",
       " 59988,\n",
       " 43608,\n",
       " 35418,\n",
       " 35421,\n",
       " 51805,\n",
       " 51810,\n",
       " 60019,\n",
       " 35447,\n",
       " 60026,\n",
       " 35451,\n",
       " 43643,\n",
       " 19072,\n",
       " 35458,\n",
       " 60049,\n",
       " 60068,\n",
       " 60069,\n",
       " 60071,\n",
       " 60081,\n",
       " 19125,\n",
       " 19126,\n",
       " 51894,\n",
       " 60085,\n",
       " 60097,\n",
       " 19138,\n",
       " 27341,\n",
       " 19152,\n",
       " 27361,\n",
       " 60145,\n",
       " 60160,\n",
       " 27406,\n",
       " 60189,\n",
       " 52005,\n",
       " 60208,\n",
       " 43831,\n",
       " 43832,\n",
       " 52030,\n",
       " 19264,\n",
       " 52037,\n",
       " 60242,\n",
       " 43866,\n",
       " 43873,\n",
       " 60267,\n",
       " 52084,\n",
       " 19321,\n",
       " 60283,\n",
       " 52093,\n",
       " 27519,\n",
       " 60289,\n",
       " 60294,\n",
       " 43911,\n",
       " 19351,\n",
       " 60323,\n",
       " 43942,\n",
       " 60330,\n",
       " 60334,\n",
       " 52144,\n",
       " 60337,\n",
       " 43954,\n",
       " 60342,\n",
       " 60344,\n",
       " 60347,\n",
       " 35777,\n",
       " 35781,\n",
       " 35786,\n",
       " 60378,\n",
       " 35804,\n",
       " 60398,\n",
       " 27636,\n",
       " 3063,\n",
       " 44023,\n",
       " 35838,\n",
       " 60426,\n",
       " 60433,\n",
       " 52242,\n",
       " 60438,\n",
       " 44060,\n",
       " 44070,\n",
       " 35879,\n",
       " 44071,\n",
       " 44074,\n",
       " 52269,\n",
       " 44097,\n",
       " 35915,\n",
       " 52300,\n",
       " 44109,\n",
       " 19535,\n",
       " 44112,\n",
       " 52304,\n",
       " 52313,\n",
       " 44123,\n",
       " 60515,\n",
       " 60518,\n",
       " 11370,\n",
       " 52345,\n",
       " 44154,\n",
       " 52346,\n",
       " 60537,\n",
       " 19584,\n",
       " 44164,\n",
       " 44168,\n",
       " 52361,\n",
       " 52362,\n",
       " 3226,\n",
       " 60590,\n",
       " 36015,\n",
       " 60595,\n",
       " 52404,\n",
       " 36021,\n",
       " 60610,\n",
       " 3282,\n",
       " 52440,\n",
       " 60632,\n",
       " 27867,\n",
       " 52443,\n",
       " 44263,\n",
       " 52459,\n",
       " 27896,\n",
       " 44282,\n",
       " 52482,\n",
       " 11524,\n",
       " 27913,\n",
       " 27916,\n",
       " 27917,\n",
       " 36114,\n",
       " 3347,\n",
       " 36116,\n",
       " 52502,\n",
       " 19737,\n",
       " 52506,\n",
       " 52509,\n",
       " 27969,\n",
       " 27971,\n",
       " 36166,\n",
       " 52553,\n",
       " 60762,\n",
       " 36196,\n",
       " 36199,\n",
       " 3433,\n",
       " 44398,\n",
       " 52591,\n",
       " 36209,\n",
       " 36213,\n",
       " 60791,\n",
       " 28033,\n",
       " 60802,\n",
       " 19845,\n",
       " 52637,\n",
       " 36256,\n",
       " 52641,\n",
       " 44450,\n",
       " 36259,\n",
       " 60835,\n",
       " 19887,\n",
       " 60847,\n",
       " 52657,\n",
       " 44466,\n",
       " 60878,\n",
       " 44509,\n",
       " 44511,\n",
       " 44512,\n",
       " 60901,\n",
       " 28138,\n",
       " 60938,\n",
       " 19987,\n",
       " 28182,\n",
       " 52770,\n",
       " 44581,\n",
       " 3622,\n",
       " 28199,\n",
       " 60971,\n",
       " 3629,\n",
       " 60977,\n",
       " 36405,\n",
       " 36415,\n",
       " 3649,\n",
       " 61015,\n",
       " 61028,\n",
       " 36454,\n",
       " 52840,\n",
       " 36466,\n",
       " 61042,\n",
       " 36475,\n",
       " 61058,\n",
       " 52867,\n",
       " 11912,\n",
       " 44688,\n",
       " 61076,\n",
       " 52886,\n",
       " 36507,\n",
       " 11939,\n",
       " 61091,\n",
       " 61131,\n",
       " 61132,\n",
       " 44765,\n",
       " 44766,\n",
       " 20191,\n",
       " 52961,\n",
       " 44774,\n",
       " 36585,\n",
       " 61168,\n",
       " 52996,\n",
       " 61189,\n",
       " 28425,\n",
       " 53004,\n",
       " 53005,\n",
       " 61196,\n",
       " 61199,\n",
       " 44836,\n",
       " 20263,\n",
       " 61233,\n",
       " 12096,\n",
       " 44875,\n",
       " 53073,\n",
       " 53074,\n",
       " 20312,\n",
       " 28505,\n",
       " 61273,\n",
       " 61276,\n",
       " 44895,\n",
       " 61284,\n",
       " 61292,\n",
       " 53111,\n",
       " 28536,\n",
       " 61304,\n",
       " 44938,\n",
       " 36749,\n",
       " 53136,\n",
       " 61334,\n",
       " 44957,\n",
       " 44972,\n",
       " 20401,\n",
       " 53171,\n",
       " 61366,\n",
       " 61382,\n",
       " 4040,\n",
       " 45000,\n",
       " 61398,\n",
       " 45015,\n",
       " 28636,\n",
       " 61406,\n",
       " 36833,\n",
       " 53226,\n",
       " 61425,\n",
       " 61447,\n",
       " 28685,\n",
       " 28692,\n",
       " 45077,\n",
       " 45084,\n",
       " 36893,\n",
       " 45087,\n",
       " 45098,\n",
       " 36911,\n",
       " 36918,\n",
       " 53313,\n",
       " 61507,\n",
       " 36952,\n",
       " 36954,\n",
       " 53346,\n",
       " 20579,\n",
       " 53351,\n",
       " 53352,\n",
       " 36969,\n",
       " 61558,\n",
       " 36989,\n",
       " 53373,\n",
       " 53376,\n",
       " 36994,\n",
       " 36999,\n",
       " 37020,\n",
       " 61600,\n",
       " 37032,\n",
       " 12460,\n",
       " 4270,\n",
       " 53439,\n",
       " 61631,\n",
       " 53448,\n",
       " 4312,\n",
       " 61661,\n",
       " 61662,\n",
       " 4325,\n",
       " 53478,\n",
       " 53481,\n",
       " 53483,\n",
       " 53494,\n",
       " 53498,\n",
       " 37126,\n",
       " 20756,\n",
       " 53528,\n",
       " 37146,\n",
       " 61727,\n",
       " 37161,\n",
       " 53545,\n",
       " 61743,\n",
       " 37168,\n",
       " 53552,\n",
       " 53563,\n",
       " 37183,\n",
       " 45375,\n",
       " 53578,\n",
       " 53580,\n",
       " 61775,\n",
       " 45394,\n",
       " 53590,\n",
       " 29016,\n",
       " 37211,\n",
       " 53605,\n",
       " 37226,\n",
       " 45421,\n",
       " 37237,\n",
       " 29052,\n",
       " 29057,\n",
       " 53642,\n",
       " 61867,\n",
       " 61898,\n",
       " 29169,\n",
       " 37362,\n",
       " 45557,\n",
       " 53753,\n",
       " 61951,\n",
       " 37380,\n",
       " 37384,\n",
       " 45578,\n",
       " 53771,\n",
       " 53779,\n",
       " 45598,\n",
       " 45609,\n",
       " 37426,\n",
       " 53813,\n",
       " 45624,\n",
       " 45629,\n",
       " 53841,\n",
       " 37460,\n",
       " 53851,\n",
       " 29276,\n",
       " 37468,\n",
       " 45661,\n",
       " 37473,\n",
       " 45688,\n",
       " 53884,\n",
       " 53919,\n",
       " 53920,\n",
       " 53925,\n",
       " 45771,\n",
       " 13005,\n",
       " 29393,\n",
       " 53991,\n",
       " 29425,\n",
       " 29429,\n",
       " 21251,\n",
       " 29454,\n",
       " 45843,\n",
       " 54041,\n",
       " 62237,\n",
       " 54055,\n",
       " 45895,\n",
       " 54089,\n",
       " 37706,\n",
       " 37707,\n",
       " 45903,\n",
       " 54096,\n",
       " 37713,\n",
       " 37717,\n",
       " 29526,\n",
       " 37733,\n",
       " 54120,\n",
       " 45944,\n",
       " 54141,\n",
       " 54157,\n",
       " 29590,\n",
       " 37783,\n",
       " 5016,\n",
       " 29595,\n",
       " 45987,\n",
       " 54179,\n",
       " 46000,\n",
       " 54193,\n",
       " 21438,\n",
       " 29638,\n",
       " 13261,\n",
       " 13262,\n",
       " 29656,\n",
       " 54238,\n",
       " 37855,\n",
       " 54244,\n",
       " 21487,\n",
       " 54261,\n",
       " 46075,\n",
       " 46077,\n",
       " 37888,\n",
       " 54274,\n",
       " 37896,\n",
       " 37901,\n",
       " 46109,\n",
       " 29733,\n",
       " 46130,\n",
       " 46132,\n",
       " 13368,\n",
       " 54335,\n",
       " 37954,\n",
       " 37965,\n",
       " 37976,\n",
       " 54360,\n",
       " 54373,\n",
       " 46183,\n",
       " 54385,\n",
       " 38012,\n",
       " 46224,\n",
       " 29845,\n",
       " 38037,\n",
       " 54427,\n",
       " 38054,\n",
       " 46268,\n",
       " 38085,\n",
       " 21703,\n",
       " 54474,\n",
       " 29899,\n",
       " 54501,\n",
       " 46316,\n",
       " 54515,\n",
       " 29945,\n",
       " 38141,\n",
       " 21780,\n",
       " 21786,\n",
       " 38183,\n",
       " 29994,\n",
       " 46380,\n",
       " 46396,\n",
       " 54593,\n",
       " 54602,\n",
       " 46418,\n",
       " 38229,\n",
       " 38230,\n",
       " 54613,\n",
       " 13681,\n",
       " 46450,\n",
       " 13689,\n",
       " 54658,\n",
       " 54661,\n",
       " 30092,\n",
       " 30103,\n",
       " 30107,\n",
       " 38304,\n",
       " 38312,\n",
       " 30122,\n",
       " 54745,\n",
       " 38362,\n",
       " 38399,\n",
       " 38402,\n",
       " 54790,\n",
       " 22025,\n",
       " 54814,\n",
       " 22055,\n",
       " 38440,\n",
       " 54846,\n",
       " 30279,\n",
       " 38472,\n",
       " 54863,\n",
       " 30293,\n",
       " 54870,\n",
       " 46682,\n",
       " 38494,\n",
       " 38500,\n",
       " 13926,\n",
       " 54886,\n",
       " 13939,\n",
       " 54900,\n",
       " 30334,\n",
       " 22143,\n",
       " 13954,\n",
       " 30338,\n",
       " 54918,\n",
       " 13959,\n",
       " 46732,\n",
       " 5786,\n",
       " 5791,\n",
       " 46766,\n",
       " 38589,\n",
       " 22209,\n",
       " 55012,\n",
       " 46829,\n",
       " 38639,\n",
       " 55023,\n",
       " 46844,\n",
       " 46846,\n",
       " 38657,\n",
       " 46851,\n",
       " 46852,\n",
       " 46854,\n",
       " 46855,\n",
       " 30473,\n",
       " 22304,\n",
       " 55081,\n",
       " 55091,\n",
       " 30516,\n",
       " 38709,\n",
       " 22327,\n",
       " 55102,\n",
       " 22382,\n",
       " 46967,\n",
       " 22399,\n",
       " 6017,\n",
       " 30610,\n",
       " 30611,\n",
       " 22427,\n",
       " 22433,\n",
       " 30631,\n",
       " 55208,\n",
       " 6057,\n",
       " 30648,\n",
       " 30663,\n",
       " 22475,\n",
       " 6100,\n",
       " 14294,\n",
       " 22486,\n",
       " 55257,\n",
       " 22493,\n",
       " 55269,\n",
       " 30694,\n",
       " 47098,\n",
       " 55294,\n",
       " 47102,\n",
       " 55306,\n",
       " 14353,\n",
       " 47125,\n",
       " 55317,\n",
       " 30746,\n",
       " 38938,\n",
       " 47133,\n",
       " 38950,\n",
       " 47149,\n",
       " 38975,\n",
       " 22594,\n",
       " 55367,\n",
       " 47186,\n",
       " 47187,\n",
       " 55413,\n",
       " 47228,\n",
       " 55422,\n",
       " 30856,\n",
       " 55433,\n",
       " 55437,\n",
       " 55439,\n",
       " 22679,\n",
       " 55453,\n",
       " 39071,\n",
       " 55455,\n",
       " 39083,\n",
       " 47288,\n",
       " 55480,\n",
       " 55484,\n",
       " 55490,\n",
       " 47302,\n",
       " 47325,\n",
       " 22763,\n",
       " 14574,\n",
       " 30969,\n",
       " 30972,\n",
       " 6430,\n",
       " 22819,\n",
       " 55589,\n",
       " 31018,\n",
       " 39216,\n",
       " 55610,\n",
       " 47422,\n",
       " 39232,\n",
       " 31058,\n",
       " 55641,\n",
       " 55642,\n",
       " 55644,\n",
       " 55653,\n",
       " 55664,\n",
       " ...}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_in_index(iidx, [\"artificial\", \"intelligence\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: finding better keywords\n",
    "\n",
    "Keywords are words that differentiate a particular document from the other documents in the collection.\n",
    "\n",
    "This means that the TFIDF measure could be useful to find keywords within a document.\n",
    "\n",
    "For such, fit a TFIDF vectorizer in the whole collection of abstracts and then experiment to find out:\n",
    "\n",
    "1. if the words with largest TFIDF in our abstract are the same as the proposed keywords\n",
    "1. if the words are meaningful towards our abstract\n",
    "1. if searching by the TFIDF-generated words could lead to better recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def k_most_relevant_words_in_context(context, k):\n",
    "    tfidf_matrix = TfidfVectorizer(min_df=2, max_df=0.2, max_features=1000, stop_words=\"english\").fit_transform(context)\n",
    "    \n",
    "    return np.argsort(tfidf_matrix.toarray()[0,:])[-k:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([520, 611, 506, 304, 720])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_most_relevant_words_in_context(df[\"abstract\"], 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: modelling abstracts with topics\n",
    "\n",
    "Remember that, in our topic model with LDA, we decompose the word count matrix as:\n",
    "\n",
    "$$\n",
    "X \\approx BA,\n",
    "$$\n",
    "\n",
    "where $B$ contains a representation of each document in terms of its topics.\n",
    "\n",
    "However, we have not discussed how to find an optimal number of topics.\n",
    "\n",
    "The idea used by [Amami et al.](http://link.springer.com/10.1007/978-3-319-41754-7_17) is to choose the number of topics that minimizes a metric called *perplexity*.\n",
    "\n",
    "Perplexity is a measure of the certainty of sampling a word using our model (see [Griffiths and Steyvers](https://doi.org/10.1073/pnas.0307752101)). Lower values are better. With too few topics, the model is in fact making very broad assumptions regarding data; with too many topics, there is a greater chance of finding data is too sparse for a relevant estimation.\n",
    "\n",
    "Modify the code below to find an optimal number of topics for our data. Then, decompose all documents in the collection (also, do it to our abstract!) using the topic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting vectorizer\n",
      "Fitting LDA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:21<01:27, 21.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of components: 2. Perplexity: 659.4180045242211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [00:40<00:59, 19.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of components: 10. Perplexity: 603.6368515894242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [01:00<00:40, 20.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of components: 20. Perplexity: 596.4369294606857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [01:22<00:20, 20.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of components: 50. Perplexity: 638.2369209586114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [01:46<00:00, 21.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of components: 100. Perplexity: 734.419984768313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tqdm import tqdm\n",
    "\n",
    "print('Fitting vectorizer')\n",
    "vectorizer = CountVectorizer(stop_words='english', min_df=10, max_df=0.8, max_features=1000).fit(df['abstract'])\n",
    "abstract_vectorized = vectorizer.transform(df['abstract'].sample(10000))\n",
    "\n",
    "print('Fitting LDA')\n",
    "for n_components in tqdm([2, 10, 20, 50, 100]):\n",
    "    lda = LatentDirichletAllocation(n_components=n_components, random_state=42, n_jobs=-1)\n",
    "    lda.fit(abstract_vectorized)\n",
    "    print(f\"Number of components: {n_components}. Perplexity: {lda.perplexity(abstract_vectorized)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: KL and JS divergences\n",
    "\n",
    "The decomposition resulting from LDA is a probability distribution. The distance between two probability distributions can be calculated using the Kullback-Leibner divergence, which is calculated by:\n",
    "\n",
    "$$\n",
    "D_{KL}(P \\parallel Q) = \\sum_{i} P(i) \\log \\left( \\frac{P(i)}{Q(i)} \\right)\n",
    "$$\n",
    "\n",
    "However, the KL divergence is not symetric, which was bothersome to Amani and their colleagues. For this reason, they used the Jensen-Shannon (JS) divergence, given by:\n",
    "\n",
    "$$\n",
    "D_{JS}(P,Q) = \\frac{D_{KL}(P \\parallel Q) + D_{KL}(Q \\parallel P)}{2}\n",
    "$$\n",
    "\n",
    "See the code below demonstrating how this works in practice:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00262729 0.42444138 0.24123015 0.00260597 0.3290952 ]]\n",
      "[[0.00400545 0.48318949 0.00402044 0.07129568 0.43748893]]\n",
      "[[0.26500287 0.0031395  0.00311942 0.72560769 0.00313053]]\n",
      "0.3155663578041288\n",
      "0.8066210162533497\n",
      "0.735833206525308\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import jensenshannon\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=5, random_state=42, n_jobs=-1)\n",
    "lda.fit(abstract_vectorized)\n",
    "\n",
    "topics1 = lda.transform(abstract_vectorized[0,:])\n",
    "topics2 = lda.transform(abstract_vectorized[1,:])\n",
    "topics3 = lda.transform(abstract_vectorized[500,:])\n",
    "\n",
    "print(topics1)\n",
    "print(topics2)\n",
    "print(topics3)\n",
    "\n",
    "print(jensenshannon(topics1.ravel(), topics2.ravel()))\n",
    "print(jensenshannon(topics1.ravel(), topics3.ravel()))\n",
    "print(jensenshannon(topics2.ravel(), topics3.ravel()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the LDA models you fitted in Exercise 4. Find the topic models for our abstract, and for each of the elements in the dataset. Then, make a function that retrieves the $K$ elements (where $K$ is an integer you can choose!) from the dataset that are closer to our abstract!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5\n",
    "\n",
    "Compare the recommendations provided by keyword searching, by TDIDF keyword searching, and by topic modelling. \n",
    "\n",
    "1. Which recommendation seems more useful?\n",
    "1. Could you combine the techniques above (at least 2 of them) to get a possibly better recommendation?\n",
    "1. Can you use an LLM to help with this task? How? Implement an LLM-based solution and compare it with the previous ones.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
